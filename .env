#####################################################
# GPU Configuration - HARDCODED
#####################################################
GPU_LAYERS=16
CTX_SIZE=32768
BATCH_SIZE=128
THREADS=6

#####################################################
# Model & Server Paths
#####################################################
MODEL_PATH="D:/_ProjectWorks/WORKENVIRONMENT/_Aud.io/crates/offline-intelligence/Resources/models/gguf-model.gguf"
LLAMA_BIN="D:/_ProjectWorks/WORKENVIRONMENT/_Aud.io/crates/offline-intelligence/Resources/bin/Windows/llama-b6970-bin-win-cuda-12.4-x64/llama-server.exe"

#####################################################
# Network Configuration
#####################################################
API_HOST=127.0.0.1
API_PORT=8000
LLAMA_HOST=127.0.0.1
LLAMA_PORT=8001

#####################################################
# Performance & Timeout Settings
#####################################################
MAX_CONCURRENT_STREAMS=2
HOT_SWAP_GRACE_SECONDS=25
HEALTH_TIMEOUT_SECONDS=600
HEALTH_CHECK_TIMEOUT_SECONDS=900
GENERATE_TIMEOUT_SECONDS=300
STREAM_TIMEOUT_SECONDS=600

#####################################################
# Monitoring & Logging
#####################################################
PROMETHEUS_PORT=9000
REQUESTS_PER_SECOND=24
RUST_LOG=info,axum=info,tower_http=info

MMPROJ_PATH=none
ENABLE_MULTIMODAL=false
USE_CLIP=false